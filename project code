#BUS 462 Group Project - Codebase
#Team E.Kappa
#April 5, 2022


#Academic integrity pledge: We pledge on our honor that we have neither received nor given unauthorized assistance on this deliverable.

#Clearing mem buffers
cat("\014")  #Clear Console
rm(list = ls(all.names = TRUE))# clear all
gc()
set.seed(42) #Set a seed to ensure repeatable random samples

#Libraries
require(data.table)
require(pastecs)
require(stargazer)
require(PerformanceAnalytics)
require(corrplot)

data <- fread("/Users/aliciachow/Documents/4th Year/BUS 462/Datasets/credit_data.csv")
colSums(is.na(data))
data <- na.omit(data)

#Change data type of variables to integers
data$checking_status <- as.integer(factor(data$checking_status, levels = c("'<0'","'0<=X<200'","'>=200'","'no checking'")))
data$credit_history <- as.integer(factor(data$credit_history, levels = c("'critical/other existing credit'", "'existing paid'",  "'delayed previously'", "'no credits/all paid'" ,"'all paid'" )))
data$purpose <- as.integer(factor(data$purpose, levels = c("'domestic appliance'","radio/tv","furniture/equipment","repairs","'used car'","'new car'","education","business","retraining","other"))) #assuming the order goes from the cheapest purchase to the most expensive purchase.
data$savings_status<- as.integer(factor(data$savings_status, levels = c("'no known savings'", "'<100'","'500<=X<1000'","'>=1000'","'100<=X<500'")))
data$employment <- as.integer(factor(data$employment, levels = c("unemployed","'<1'","'1<=X<4'","'4<=X<7'","'>=7'")))
data$personal_status <- as.integer(factor(data$personal_status, levels = c("'male single'","'female div/dep/mar'","'male div/sep'","'male mar/wid'")))
data$other_parties <- as.integer(factor(data$other_parties, levels = c("none","guarantor","'co applicant'")))
data$property_magnitude <- as.integer(factor(data$property_magnitude, levels = c("'real estate'","'life insurance'","'no known property'","car")))
data$other_payment_plans<- as.integer(factor(data$other_payment_plans, levels = c("none", "bank","stores")))
data$housing<- as.integer(factor(data$housing, levels = c("own", "'for free'","rent")))
data$job<- as.integer(factor(data$job, levels = c("skilled", "'unskilled resident'","'high qualif/self emp/mgmt'","'unemp/unskilled non res'")))
data$own_telephone<- as.integer(factor(data$own_telephone, levels = c("none","yes")))
data$foreign_worker<- as.integer(factor(data$foreign_worker, levels = c("yes","no")))
data$class<- as.integer(factor(data$class, levels = c("good","bad")))

#Q1 - What variables should be focused on while evaluating credit amount?

#To get the correlation chart of all variables.
chart.Correlation(data, histogram=TRUE, pch=19)

# OLS model 
m.OLS<-lm(credit_amount ~ ., data=data)

# Looking at the summary results from the OLS regression model
summary(m.OLS)
AIC(m.OLS)
stargazer(m.OLS,type="text")

#Stepwise Regression
model.step.OLS <- step(m.OLS) 

# Looking at the summary results from the Stepwise regression 
stargazer(m.OLS,model.step.OLS,type="text")
summary(model.step.OLS)
AIC(model.step.OLS)

#Q2 - How does the scale of purchases affect one’s credit amount? That is, are individuals more likely to pay off their credit or less likely when purchasing larger items (which may take longer to pay off)?
#Dependent Variable: Credit Amount
#Independent Variables: Purpose, Credit History, Duration, Property Magnitude, Housing, Telephone (assuming it is similar to a mobile device and will be quite expensive)

# Begin by subsetting our data with only the relevant variables (dependent and independent variables)
dataq2<-subset(data,select=c(purpose, credit_history,duration,property_magnitude,housing,own_telephone,credit_amount))

# Run a descriptive statistics on our new subset
stat.desc(dataq2)

#To get the correlation chart of all variables.
chart.Correlation(dataq2, histogram=TRUE, pch=19)

# We can also visualize the correlations utilizing a correlogram
cor_data<-cor(dataq2)
corrplot(cor_data, method="number", "upper")

# To analyze the data and answer our question, run the OLS model with all independent variables mentioned above
OLSModel1 <- lm(credit_amount ~ purpose + credit_history + duration + property_magnitude + housing + own_telephone, data=dataq2)

# Let's look at the summary results from the OLS regression model
summary(OLSModel1)
stargazer(OLSModel1,type="text")
AIC(OLSModel1)

# Visualize the relationship between the amount of credit an individual has/duration and the various purchasing habits (purpose)
plot(dataq2$purpose, dataq2$credit_amount, main="Purpose vs. Credit Amount", xlab = "Purpose", ylab = "Credit Amount")
plot(dataq2$purpose, dataq2$duration, main="Purpose vs. Duration", xlab = "Purpose", ylab = "Duration")
plot(dataq2$duration, dataq2$credit_amount, main="Duration vs. Credit Amount", xlab = "Duration", ylab = "Credit Amount")

# Based on the first OLS Model, duration is the most significant variable when analyzing one's credit amount
# For our question, we want to analyze how purchases impact an individual's credit amount, and whether duration plays a factor in this 
# We should find the interactions between duration and purpose, and duration and owning a phone. 


# Create a variable that considers the interaction between purpose and duration
purpose_duration <- (dataq2$purpose*dataq2$duration)

#Create a variable that considers the interaction between own_telephone and duration
telephone_duration <- (dataq2$own_telephone*dataq2$duration)

#Now create an OLS model that considers BOTH these interactions to see which best explains credit amounts
OLSModel2 <- lm(credit_amount ~ property_magnitude + housing + purpose_duration + telephone_duration, data=dataq2)
OLSModel2

summary(OLSModel2)
stargazer(OLSModel2, type="text")
AIC(OLSModel2)

stargazer(OLSModel1, OLSModel2, type="text")


# While housing has a high co-efficient and has a low correlation with duration and credit amount, our independent variable
cor(dataq2$credit_history,dataq2$duration)
cor(dataq2$credit_amount,dataq2$housing)

# Hypothetically, if we ran an OLS regression using housing
housing_duration<-(dataq2$housing*dataq2$duration)
OLSModel3 <- lm(credit_amount ~ property_magnitude + housing_duration + purpose + own_telephone + credit_history, data=dataq2)
OLSModel3
summary(OLSModel3)
stargazer(OLSModel3, type="text")

# The adjusted r-squared value is ~0.3 which is fairly low compared to the other models. 
stargazer(OLSModel1, OLSModel2, OLSModel3, type="text")


#Q3 - Given the loan/customer features, we want to predict the default on loans.
head(data)
tail(data)

#Convert remaining data types
data$personal_status <- as.integer(factor(data$personal_status, levels = c("'female div/dep/mar'","'male div/sep'","'male mar/wid'","	
'male single'")))
data$other_parties <- as.integer(factor(data$other_parties, levels = c("none","guarantor","co applicant")))
data$other_payment_plans <- as.integer(factor(data$other_payment_plans, levels = c("none","bank","stores")))
data$job <- as.integer(factor(data$job, levels = c("skilled","‘unskilled resident’","‘high qualif/self emp/mgmt’","‘unemp/unskilled non res'")))
data$foreign_worker <- as.integer(factor(data$foreign_worker, levels = c("yes","no")))
data$employment <- as.integer(factor(data$employment, levels = c("'<1'","'>=7'","'1<=X<4'","unemployed")))

#Subset data
dataq3 <- data[,c("purpose","other_parties","installment_commitment","property_magnitude", "existing_credits", "num_dependents", "personal_status", "employment", "residence_since", "age", "housing", "job", "own_telephone", "foreign_worker", "other_payment_plans")]
dataframeq3 <- data.frame(dataq3)

#Stats summary
stat.desc(dataq3)
stargazer(dataq3,type="text")

#Correlation
chart.Correlation(dataq3)

#Shuffle data
shuffle_index <- sample(1:nrow(dataq3))
dataq3 <- dataq3[shuffle_index,]

#Create test and train data
#Top 80% for train and bottom 20% for test
n_cut <- round(nrow(dataq3)*.8,0) # row at which to cut data
dataq3_train <- dataq3[1:n_cut] # create training data subset
dataq3_test <- dataq3[(n_cut+1):nrow(dataq3)] # create test data subset

#Check dim
dim(dataq3_train)
dim(dataq3_test)

#Test distribution
#prop.table(table(dataq3_train$))
#prop.table(table(data_testq3$))

require(data.table)
install.packages('dplyr')
install.packages('fastDummies')
install.packages('caret')
library(caret)
library(fastDummies)
library(dplyr)
library(ggplot2)

data$BAD = ifelse(data$class=='bad', 1, 0)

data$purpose = factor(data$purpose)
data$checking_status = factor(data$checking_status)
data$credit_history = factor(data$credit_history)
data$savings_status = factor(data$savings_status)
data$employment = factor(data$employment)
data$property_magnitude = factor(data$property_magnitude)
data$housing = factor(data$housing)
data$job  = factor(data$job)
data$foreign_worker = factor(data$foreign_worker)

## checking_status
ggplot(data = data, mapping = aes(x = checking_status, fill = class)) + geom_bar(position = 'stack')

##we see that some factors above have a higher number of defaults (Even proportionally)

#credit_history
ggplot(data = data, mapping = aes(x = credit_history, fill = class)) + geom_bar(position = 'stack')

#purpose
ggplot(data = data, mapping = aes(x = purpose, fill = class)) + geom_bar(position = 'fill') + labs(y = 'proportion') 

## education, new car and others have higher default rates


#own telephone

ggplot(data = data, mapping = aes(x = own_telephone, fill = class)) + geom_bar(position = 'fill') + labs(y = 'proportion') 
## does not explain much

ggplot(data = data, mapping = aes(x = employment, fill = class)) + geom_bar(position = 'dodge')  

## employment does not have a lot fo explanatory power, except that it tells us that newly hired persons have a higher default rate

# foreign worker
ggplot(data = data, mapping = aes(x = foreign_worker, fill = class)) + geom_bar(position = 'fill') + labs(y = 'proportion')

## foreign workers default more

ggplot(data = data, mapping = aes(x = credit_amount, fill = class)) + geom_density(alpha = 0.4)
## shows that smaller loans are easily paid and that there are more defaulters with high credit amounts

ggplot(data = data, mapping = aes(x = class, y = age)) + geom_boxplot()
# the distributions are not that different, however younger people tend to have a higher mean default rate

ggplot(data = data, mapping = aes(x = num_dependents, fill = class)) + geom_bar(position = 'fill') + labs(y = 'proportion')

# number of dependents does not explain anything

# time to check some interactions

# credit_amount and duration

ggplot(data, mapping = aes(x = duration,  y= credit_amount, color = class)) + geom_point() + geom_smooth(method = 'gam', se = FALSE)

## not too different

# credit amount and age
ggplot(data, mapping = aes(x = age,  y= credit_amount, color = class)) + geom_point() + geom_smooth(method = 'gam', se = FALSE)

## different distributions, therefore we should create an interaction variable

data$age_amount = data$age * data$credit_amount




# experiment 1
AUC1 = c()
accuracy1 = c()
for (k in 1:5) {
  train_idx = createDataPartition(data$class, p = 0.85, list = FALSE)
  df_train = data[train_idx,]
  df_test = data[-train_idx,]
  model <- glm(BAD ~ checking_status + duration + credit_history + credit_amount + purpose + credit_amount +
                 installment_commitment + property_magnitude +
                 age + employment +  housing + job + foreign_worker + age_amount, data = df_train,
               family = binomial(link = 'logit'))
  glm_probs = predict(model, newdata = df_test, type = 'response', se.fit = TRUE)
  
  predictions = ifelse(glm_probs$fit > 0.5, 1, 0)
  
  accuracy1 = c(accuracy1, sum(df_test$BAD == predictions)/length(predictions))
  
  ## AUC is a ranking measure 
  AUC1 = c(AUC1,ModelMetrics::auc(actual = df_test$BAD, predicted = as.numeric(glm_probs$fit)))
  
}
nrow(df_test)
predictions


mean(accuracy1)
mean(AUC1)

#prob(BAD) = y = f(age + a.....)
 # the function in experiment one is estimated via logistic regression
# the function in experiment three is estimated via CART



# experiment 2: checking how AUC and accuracy change with the size of training set

set.seed(123)
AUC2 = c()
accuracy2 = c()
for (p in c(0.6, 0.65, 0.7, 0.75,0.77, 0.8, 0.83, 0.85, 0.9, 0.91, 0.93, 0.95)) {
  train_idx = createDataPartition(data$class, p = p, list = FALSE)
  df_train = data[train_idx,]
  df_test = data[-train_idx,]
  logit <- glm(BAD ~ checking_status + duration + credit_history + credit_amount + purpose + credit_amount +
                 installment_commitment + property_magnitude +
                 age + employment +  housing + job + foreign_worker + age_amount, data = df_train,
               family = binomial(link = 'logit'))
  glm_probs = predict(logit, newdata = df_test, type = 'response', se.fit = TRUE)
  
  predictions = ifelse(glm_probs$fit > 0.6, 1, 0)
  
  accuracy2 = c(accuracy2, sum(df_test$BAD == predictions)/length(predictions))
  
  ## AUC is a ranking measure 
  AUC2 = c(AUC2,ModelMetrics::auc(actual = df_test$BAD, predicted = as.numeric(glm_probs$fit)))
  
}


p_auc_acc = data.frame('p' = c(0.6, 0.65, 0.7, 0.75, 0.77, 0.8, 0.83, 0.85, 0.9, 0.91, 0.93, 0.95), 'auc' = AUC2, 'accuracy' = accuracy2)
ggplot(data = p_auc_acc, mapping = aes(x=p, y = auc)) + geom_smooth()
ggplot(data = p_auc_acc, mapping = aes(x=p, y = accuracy)) + geom_smooth()


# experiment 3: can CART do better?


set.seed(123)
auc3 = c()
for (p in c(0.6, 0.65, 0.7, 0.75,0.77, 0.8, 0.83, 0.85, 0.9, 0.91, 0.93, 0.95)) {
  train_idx = createDataPartition(data$class, p =p, list = FALSE)
  df_train = data[train_idx,]
  df_test = data[-train_idx,]
  cart <- rpart::rpart(BAD ~ checking_status + duration + credit_history + credit_amount + purpose + credit_amount +
                         installment_commitment + property_magnitude +
                         age + employment +  housing + job + foreign_worker + age_amount, data = df_train, method = 'class', minbucket = 10)
  
  
  predictions_CART = predict(cart, newdata = df_test, type = 'prob')
  
  auc3 = c(auc3, ModelMetrics::auc(actual = df_test$BAD, predicted = predictions_CART[,2]))
  
  }  

p_auc_acc$auc_cart = auc3

ggplot() + geom_smooth(data = p_auc_acc, mapping = aes(x=p, y = auc, col = 'auc_logit')) + geom_smooth(data = p_auc_acc, mapping = aes(x=p, y = auc_cart, col = 'auc_CART'))




## divide data set into train and test
## trained (both) models on train and tested them on test set (in terms of auc)
